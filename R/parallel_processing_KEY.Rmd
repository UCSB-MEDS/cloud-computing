---
title: "Parallel Processing in R"
author: Sam Csik
date: 'April 18, 2022'
output: 
  github_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = FALSE)
```

*Much of the following code and content was borrowed/adapted from Danielle Ferraro's R-Ladies SB workshop, [A Gentle Introduction to Parallel Processing in R](https://danielleferraro.github.io/rladies-sb-parallel/#1)...who borrowed code/exercises from Grant McDermott's lecture notes, [Parallel programming](https://raw.githack.com/uo-ec607/lectures/master/12-parallel/12-parallel.html). Code in part III (Using the `{foreach}` package), also comes from Blas M. Benito's blog post, [Parallelized loops with R](https://www.blasbenito.com/post/02_parallelizing_loops_with_r/). Thanks to Danielle, Grant, and Blas for putting together some awesome teaching resources!*

### Things to remember before getting started

-   **Use parallel processing when** (a) your task is embarrassingly parallel i.e. if your analysis could easily be separated into many identical but separate tasks that do not rely on one another, and (b) when your tasks are computationally intensive and take time to complete.

-   **Do not use parallel processing if** (a) it's not the right tool (e.g. maybe you're memory limited), or (b) it might not be efficient (there is computational overhead for setting up, maintaining, and terminating multiple processors --- for small processes, the overhead cost may be greater than the speedup)

-   Always **profile** your code first (i.e. find the bottleneck and try to eliminate it)

-   Parallel code can be implemented in two ways: **forking** or **sockets** (we'll explore both below)

    -   **forking** = faster but doesn't work on Windows machines and may cause problems in IDEs (works by copying the entire current version of R and moving it to a new core)

    -   **sockets** = a bit slower but works across operating systems and in IDEs (works by launching a new version of R on each core)

### 0. Install & load packages

#### Install packages (if needed)

```{r, install packages}
# install.packages(c("tidyverse", "tictoc", "foreach", "doParallel"))
```

#### Load packages

```{r}
library(tidyverse) # for some data wrangling
library(tictoc) # for timing how long our code takes to run
library(parallel) # a base R package that supplies parallel versions of the `apply` functions
library(foreach) # allow for parallelization of for loops
library(doParallel) # provides `parallel` backend; used in conjunction with `foreach`; `{doFuture}` provides the futures backend
```

### I. Create a sloowwww function (for testing)

This `slow_square()` function calculates the square of a number, saves the value and the corresponding squared value to a data frame, then put R to sleep for 2 seconds.

```{r}
slow_square <- function(x) {
  x_squared <- x^2 
  d <- tibble(value = x, 
              value_squared = x_squared)
  Sys.sleep(2)
  return(d)
}
```

### II. Using the `{parallel}` package

**The `{parallel}` package provides parallel versions of base R `apply()` functions**

#### Detecting cores

First, use `parallel::detectCores()` to determine how many cores your computer has (mine has 8)

```{r}
detectCores()
```

**You decide how many cores to use** when parallel processing, though many suggest **reserving one core** for running other background processes. For extremely computationally-intensive tasks, you may choose to shut down all other applications on your computer and use all available cores. Remember, there is **computational overhead** for setting up, maintaining, and terminating multiple processors, so *take care when using computers/servers with many cores available.*

#### `lapply()` vs. `mclapply()` 

You may already be familiar with the base R function, `lapply()`, which applies a function over the elements of a list or vector. `mcapply()` is the parallel equivalent. Let's test both using our `slow_square()` function.

First up, `lapply()` which takes two arguments, a vector `X` and a function, `FUN`, to be applied over each element of `X`. We'll bind the results into a tibble using `bind_rows()`.

```{r, using lapply}
tic("using lapply()")
lapply(X = 1:5, FUN = slow_square) %>% 
  bind_rows()
toc()
```

`lapply()`, which runs serially using 1 core, took (me) **10.686 sec**

Next up is `mclapply()`, which takes the same first two arguments as `lapply()`, `X` and `FUN`. We'll also specify the number of cores to use with `mc.cores`. **REMINDER: `mclapply()` uses *forking*, which will only work for Mac users. Windows users, see the next section on `parLapply()`, which is OS agnostic**

```{r, using mclapply}
#.................your total number of cores - 1.................
n_cores <- detectCores() - 1 

#................parallelize using forking method................
tic("using mclapply()")
mclapply(X = 1:5, FUN = slow_square, mc.cores = n_cores) %>% 
  bind_rows()
toc()
```

`mclapply()`, which ran our code in parallel using the forking method and n-1 cores (for me, that's 7 cores) took **2.819 sec**

#### `parLapply()`

Windows users will need to use the `parLapply()` function which uses the *socket* method. This requires a bit of additional setup (Mac users can run this code as well). The `parLapply()` function takes three main arguments: your cluster, `cl`, a vector, `x`, and a function, `fun`, to be applied over each element of `x`

```{r, eval = FALSE}
#......................some necessary setup......................
cluster <- makePSOCKcluster(n_cores) # make a local cluster (i.e. a collection of cores on our computer)
clusterEvalQ(cluster, library("tibble")) # export necessary pkgs into our cluster 

#................parallelize using socket method.................
tic("using parLapply()")
parLapply(cluster, 1:5, slow_square) %>% 
  bind_rows()
toc()
```

`parLapply()`, which ran our code in parallel using the socket method and n-1 cores (for me, that's 7 cores) was only slightly slower than `mclapply()` at **2.886 sec**

### III. Using the `{foreach}` package

**The `{foreach}` package works on all operating systems and can be implemented using either the forking or socket method. The primary function, `foreach()` enables the use of loops rather than the apply format**

Some initial setup is required -- first, use the `parallel:makeCluster()` function to define your computer's cluster. Next, register your backend -- here we'll use the `doParallel` package to use `parallel` as a backend (alternatively, you can use the `doFuture` package to use `future` as a backend)
```{r}
#......................create your cluster.......................
my_cluster <- parallel::makeCluster(spec = n_cores, type = "PSOCK") # default type = "PSOCK"; alt = "FORK"

#............OPTIONAL: check your cluster definition.............
print(my_cluster)

#............register the parallel processing backend............
doParallel::registerDoParallel(cl = my_cluster)

#........OPTIONAL: check that your backend is registered.........
foreach::getDoParRegistered()
```

The `foreach()` syntax is a bit different:

`foreach(...) %dopar% {...}`

We supply the `foreach()` function with the values we want to iterate over, `i`. By default, `foreach()` returns results as a list -- the `.combine` argument, allows us to specify a function to return results as a different class (here, we use `bind_rows()` to return a tibble). The `%dopar%` operator indicates that the computation will be run in parallel and the function we want to apply follows inside `{}`. **Note:** the `%do%` operator can be used in place of `%dopar%` to revert the expression back to serial processing, which may be useful for debugging.
```{r}
tic("using foreach() (with parallel backend)")
foreach(i = 1:5, .combine = bind_rows) %dopar% {slow_square(i)}
toc()
```

How long did it take when you used forking (`type = FORK`) vs sockets (`type = PSOCK`) (reminder: only Mac users can use forking)? **Note:** you can register futures as your backend by using the `DoFuture`package rather than `DoParallel`

It's recommended that you stop your cluster once you're done working with it.
```{r}
parallel::stopCluster(cl = my_cluster)
```

### IV. Other packages

Check out Danielle Ferraro's, talk [A Gentle Introduction to Parallel Processing in R](https://danielleferraro.github.io/rladies-sb-parallel/#1) to learn more about the `furrr` and `future.apply` packages, which provide parallel versions of `purrr:map()` and base R `apply()` functions, respectively, both using `future` as a backend.